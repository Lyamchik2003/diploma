{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Baseline\n",
    "\n",
    "This notebook shows how to train the baseline model for this competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(1, os.path.realpath(os.path.pardir))\n",
    "\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "from utils.train import TrainConfig, run_train_model\n",
    "from utils.augmentations import get_default_transform\n",
    "from utils import creating_dataset\n",
    "\n",
    "# this is the implementation of the custom baseline model\n",
    "from utils import hvatnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define trainer configuration\n",
    "\n",
    "The `TrainConfig` class is used to train the baseline model - have a look at the parameters it has!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = TrainConfig(exp_name='test_05_11_run_fedya', p_augs=0.3, batch_size=64, eval_interval=150, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting val datasets\n",
      "Number of moves: 72 | Dataset: fedya_tropin_standart_elbow_left\n",
      "Reorder this dataset fedya_tropin_standart_elbow_left True\n",
      "Getting train datasets\n",
      "Number of moves: 72 | Dataset: fedya_tropin_standart_elbow_left\n",
      "Reorder this dataset fedya_tropin_standart_elbow_left True\n",
      "Number of moves: 70 | Dataset: valery_first_standart_elbow_left\n",
      "Reorder this dataset valery_first_standart_elbow_left True\n",
      "Number of moves: 135 | Dataset: alex_kovalev_standart_elbow_left\n",
      "Reorder this dataset alex_kovalev_standart_elbow_left True\n",
      "Number of moves: 72 | Dataset: anna_makarova_standart_elbow_left\n",
      "Reorder this dataset anna_makarova_standart_elbow_left True\n",
      "Number of moves: 62 | Dataset: artem_snailbox_standart_elbow_left\n",
      "Reorder this dataset artem_snailbox_standart_elbow_left True\n",
      "Number of moves: 144 | Dataset: matthew_antonov_standart_elbow_left\n",
      "Reorder this dataset matthew_antonov_standart_elbow_left True\n",
      "Number of moves: 144 | Dataset: misha_korobok_standart_elbow_left\n",
      "Reorder this dataset misha_korobok_standart_elbow_left True\n",
      "Number of moves: 71 | Dataset: nikita_snailbox_standart_elbow_left\n",
      "Reorder this dataset nikita_snailbox_standart_elbow_left True\n",
      "Number of moves: 144 | Dataset: petya_chizhov_standart_elbow_left\n",
      "Reorder this dataset petya_chizhov_standart_elbow_left True\n",
      "Number of moves: 12 | Dataset: polina_maksimova_standart_elbow_left\n",
      "Reorder this dataset polina_maksimova_standart_elbow_left True\n",
      "Number of moves: 144 | Dataset: sema_duplin_standart_elbow_left\n",
      "Reorder this dataset sema_duplin_standart_elbow_left True\n",
      "Number of moves: 136 | Dataset: alex_kovalev_standart_elbow_right\n",
      "Number of moves: 69 | Dataset: andrew_snailbox_standart_elbow_right\n",
      "Number of moves: 132 | Dataset: anna_makarova_standart_elbow_right\n",
      "Number of moves: 67 | Dataset: artem_snailbox_standart_elbow_right\n",
      "Number of moves: 68 | Dataset: matthew_antonov_standart_elbow_right\n",
      "Number of moves: 72 | Dataset: matvey_gorbenko_standart_elbow_right\n",
      "Number of moves: 144 | Dataset: misha_korobok_standart_elbow_right\n",
      "Number of moves: 55 | Dataset: nikita_snailbox_standart_elbow_right\n",
      "Number of moves: 142 | Dataset: petya_chizhov_standart_elbow_right\n",
      "Number of moves: 54 | Dataset: polina_maksimova_standart_elbow_right\n",
      "Number of moves: 139 | Dataset: sema_duplin_standart_elbow_right\n",
      "Number of trainining sessions: 22\n",
      "Number of validation sessions: 1\n",
      "Size of the input (8, 256) || Size of the output (20, 32)\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = r\"C:\\Users\\nodos\\Desktop\\ДИПЛОМ\\Аня диплом\\dataset_v2_blocks\\dataset_v2_blocks\"\n",
    "\n",
    "def count_parameters(model): \n",
    "    n_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    n_total = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total: {n_total/1e6:.2f}M, Trainable: {n_trainable/1e6:.2f}M\")\n",
    "    return n_total, n_trainable\n",
    "\n",
    "\n",
    "    \n",
    "## Data preparation\n",
    "transform = get_default_transform(train_config.p_augs)\n",
    "data_paths = dict(datasets=[DATA_PATH],\n",
    "                    hand_type = ['left', 'right'], # [left, 'right']\n",
    "                    human_type = ['health', 'amputant'], # [amputant, 'health']\n",
    "                    test_dataset_list = ['fedya_tropin_standart_elbow_left'])\n",
    "data_config = creating_dataset.DataConfig(**data_paths)\n",
    "train_dataset, test_dataset = creating_dataset.get_datasets(data_config, transform=transform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the model\n",
    "As you can see below, the model has a number of hyperparameters specifying its architecture and parameters. These are the parameters used to generate the baseline predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 4210788\n",
      "Total: 4.21M, Trainable: 4.21M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4210788, 4210788)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config = hvatnet.Config(n_electrodes=8, n_channels_out=20,\n",
    "                            n_res_blocks=3, n_blocks_per_layer=3,\n",
    "                            n_filters=128, kernel_size=3,\n",
    "                            strides=(2, 2, 2), dilation=2, \n",
    "                            small_strides = (2, 2))\n",
    "model = hvatnet.HVATNetv3(model_config)\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New version for Аня"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 3224676\n",
      "Total: 3.22M, Trainable: 3.22M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3224676, 3224676)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config = hvatnet.Config(n_electrodes=8, n_channels_out=20,\n",
    "                            n_res_blocks=5, n_blocks_per_layer=2,\n",
    "                            n_filters=128, kernel_size=3,\n",
    "                            strides=(2, 2, 2), dilation=3, \n",
    "                            small_strides = (1, 1))\n",
    "model = hvatnet.HVATNetv3(model_config)\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the predictions are downsampled at 25Hz from the data originally recorded at 200Hz. The `hvatnet` model used here, automatically and correctly downsamples the data during predictions. Make sure that your model's oputput is also downsampled!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (8, 256), Y shape: (20, 32)\n",
      "Predictions shape: (20, 32)\n"
     ]
    }
   ],
   "source": [
    "X, Y = train_dataset[0]\n",
    "print(f\"X shape: {X.shape}, Y shape: {Y.shape}\")\n",
    "\n",
    "Y_hat = model(torch.tensor(X).unsqueeze(0)).squeeze().detach().numpy()\n",
    "\n",
    "print(f\"Predictions shape: {Y_hat.shape}\")\n",
    "\n",
    "assert Y.shape == Y_hat.shape, \"Predictions have the wrong shape!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code trains the baseline model using training code defined in `utils`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed initialization of scheduler\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 150: 0.315540611743927\n",
      "val loss: 0.3332122266292572\n",
      "saved model:  step_150_loss_0.3332.safetensors\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 300: 0.28197020292282104\n",
      "val loss: 0.32983162999153137\n",
      "saved model:  step_300_loss_0.3298.safetensors\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 450: 0.28486546874046326\n",
      "val loss: 0.34148386120796204\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 600: 0.2590082287788391\n",
      "val loss: 0.3368873596191406\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 750: 0.23607316613197327\n",
      "val loss: 0.3242117762565613\n",
      "saved model:  step_750_loss_0.3242.safetensors\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 900: 0.25570711493492126\n",
      "val loss: 0.32411760091781616\n",
      "saved model:  step_900_loss_0.3241.safetensors\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 1050: 0.26466646790504456\n",
      "val loss: 0.3068230450153351\n",
      "saved model:  step_1050_loss_0.3068.safetensors\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 1200: 0.2733089327812195\n",
      "val loss: 0.3151107132434845\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 1350: 0.2449377030134201\n",
      "val loss: 0.3179585635662079\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 1500: 0.2731020450592041\n",
      "val loss: 0.32577070593833923\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 1650: 0.2502213418483734\n",
      "val loss: 0.29323118925094604\n",
      "saved model:  step_1650_loss_0.2932.safetensors\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 1800: 0.22336339950561523\n",
      "val loss: 0.3127564787864685\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 1950: 0.2380904257297516\n",
      "val loss: 0.3209913372993469\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 2100: 0.21126136183738708\n",
      "val loss: 0.31052711606025696\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 2250: 0.26030611991882324\n",
      "val loss: 0.29021331667900085\n",
      "saved model:  step_2250_loss_0.2902.safetensors\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 2400: 0.2632804811000824\n",
      "val loss: 0.2922670245170593\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 2550: 0.2387726753950119\n",
      "val loss: 0.3381694257259369\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 2700: 0.22184662520885468\n",
      "val loss: 0.30248701572418213\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 2850: 0.223528653383255\n",
      "val loss: 0.2794935405254364\n",
      "saved model:  step_2850_loss_0.2795.safetensors\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 3000: 0.23379722237586975\n",
      "val loss: 0.3135973811149597\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 3150: 0.216909259557724\n",
      "val loss: 0.2864181697368622\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 3300: 0.22367998957633972\n",
      "val loss: 0.2873324155807495\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 3450: 0.23141691088676453\n",
      "val loss: 0.29501965641975403\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 3600: 0.1918526440858841\n",
      "val loss: 0.283064067363739\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 3750: 0.21995334327220917\n",
      "val loss: 0.2898559868335724\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 3900: 0.22560754418373108\n",
      "val loss: 0.28197354078292847\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 4050: 0.24382562935352325\n",
      "val loss: 0.27470606565475464\n",
      "saved model:  step_4050_loss_0.2747.safetensors\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 4200: 0.2144729197025299\n",
      "val loss: 0.3048597276210785\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 4350: 0.21588218212127686\n",
      "val loss: 0.2854617238044739\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 4500: 0.22308936715126038\n",
      "val loss: 0.28182193636894226\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 4650: 0.2084941416978836\n",
      "val loss: 0.28776609897613525\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 4800: 0.21292047202587128\n",
      "val loss: 0.2755884826183319\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 4950: 0.19364511966705322\n",
      "val loss: 0.28373396396636963\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 5100: 0.21415011584758759\n",
      "val loss: 0.28403523564338684\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 5250: 0.1982596218585968\n",
      "val loss: 0.28048327565193176\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 5400: 0.22522421181201935\n",
      "val loss: 0.2914099097251892\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 5550: 0.20053084194660187\n",
      "val loss: 0.2965748608112335\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 5700: 0.20497968792915344\n",
      "val loss: 0.2916463613510132\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 5850: 0.19730864465236664\n",
      "val loss: 0.2776235342025757\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 6000: 0.21569564938545227\n",
      "val loss: 0.28370219469070435\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 6150: 0.2237238883972168\n",
      "val loss: 0.2760131061077118\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 6300: 0.19095809757709503\n",
      "val loss: 0.29107582569122314\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 6450: 0.24108299612998962\n",
      "val loss: 0.30008813738822937\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 6600: 0.2044714391231537\n",
      "val loss: 0.2789401412010193\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 6750: 0.2680697441101074\n",
      "val loss: 0.28846094012260437\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 6900: 0.19149760901927948\n",
      "val loss: 0.2880040407180786\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 7050: 0.18552717566490173\n",
      "val loss: 0.2882615625858307\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 7200: 0.1950368583202362\n",
      "val loss: 0.28563129901885986\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 7350: 0.21030423045158386\n",
      "val loss: 0.28599193692207336\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 7500: 0.18812377750873566\n",
      "val loss: 0.291360080242157\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 7650: 0.21642455458641052\n",
      "val loss: 0.2845798134803772\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 7800: 0.22100457549095154\n",
      "val loss: 0.2743239998817444\n",
      "saved model:  step_7800_loss_0.2743.safetensors\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 7950: 0.20666590332984924\n",
      "val loss: 0.29475292563438416\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 8100: 0.23984761536121368\n",
      "val loss: 0.28836601972579956\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 8250: 0.22075514495372772\n",
      "val loss: 0.29441362619400024\n",
      "\n",
      "\n",
      "*****************"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mrun_train_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\PycharmProjects\\amputants\\utils\\train.py:113\u001b[0m, in \u001b[0;36mrun_train_model\u001b[1;34m(model, datasets, config, device)\u001b[0m\n\u001b[0;32m    111\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    112\u001b[0m loss, _ \u001b[38;5;241m=\u001b[39m model(inputs, labels)\n\u001b[1;32m--> 113\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mgrad_clip:\n\u001b[0;32m    116\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), config\u001b[38;5;241m.\u001b[39mgrad_clip)\n",
      "File \u001b[1;32mc:\\Users\\nodos\\PycharmProjects\\amputants\\.venv\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nodos\\PycharmProjects\\amputants\\.venv\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nodos\\PycharmProjects\\amputants\\.venv\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "run_train_model(model, (train_dataset, test_dataset), train_config, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
